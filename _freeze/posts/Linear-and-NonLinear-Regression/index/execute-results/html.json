{
  "hash": "15bb2eae9b4f0ff1aa4f07c616419552",
  "result": {
    "markdown": "---\ntitle: \"Linear and Non Linear Regression Blog\"\nauthor: \"Trisha Bora\"\ndate: \"2023-12-06\"\ncategories: [Regression]\nimage: \"image.jpg\"\n---\n\n# Title: Linear and Non Linear Regression.\n# Linear regression:\nLinear regression stands as the cornerstone of predictive modeling. It's a method that seeks to establish a linear relationship between a dependent variable and one or more independent variables. This relationship is expressed through a linear equation, allowing us to make predictions based on the observed data. Performing linear regression involves several steps. Here's a general outline of the process:\n\nCollect Data: A dataset with paired observations for the dependent and independent variables is desired.\n\nExplore the Data: We need to understand the characteristics of the collected data. This may involve plotting the data, checking for outliers, and examining the distribution of variables.\n\nDefine Variables: Identify the dependent variable and the independent variables.\n\nSplit the Data: Divide your dataset into two subsets: one for training the model and one for testing the model. The training set is used to fit the model, and the testing set is used to evaluate its performance.\n\nBuild the Model: For simple linear regression (one independent variable), the model is represented as: y=mx+b. Use statistical methods or optimization algorithms (such as the least squares method) to find the coefficients that minimize the difference between predicted and actual values in the training set.\n\nEvaluate the Model: Use the testing set to assess the model's performance. Common evaluation metrics include mean squared error (MSE), root mean squared error (RMSE), and R-squared.\n\nInterpret the Results: Analyze the coefficients to understand the relationship between the variables. A positive coefficient indicates a positive relationship, and a negative coefficient indicates a negative relationship.\n\nMake Predictions: Once the model's performance is satisfactory, predictions can be made on new data.\n\nValidate Assumptions: Check whether the assumptions of linear regression are met. These assumptions include linearity, independence of errors, homoscedasticity (constant variance of errors), and normality of errors.\n\nMachine Learning Code for Linear Regression: Let's dive into the practical aspect with a Python script using the ubiquitous scikit-learn library. \nThe California housing dataset is condidered.\n\nThis code crafts a narrative of linear regression using the California housing dataset, illuminating the relationship between selected features and house prices.\n\nLinear regression is a powerful tool for predicting numerical values, but when dealing with datasets that have a large number of features, the risk of overfitting becomes significant. In such cases, regularization techniques like Ridge and Lasso Regression come to the rescue by introducing penalty terms to the cost function. In this blog post, we'll delve into the California Housing dataset and compare the performance of Ridge and Lasso Regression.\n\nThe California Housing Dataset:\n\nThe California Housing dataset is a widely used dataset for regression tasks. It contains housing data for districts in California, including features such as median income, housing median age, average rooms, etc. Our goal is to predict the median house value for California districts.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n# Load the California Housing dataset from scikit-learn\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n\ncalifornia_housing = fetch_california_housing(as_frame=True)\ncalifornia_df = california_housing.frame\n\n# Select features and target\nfeatures = california_df.drop('MedHouseVal', axis=1)  # 'MedHouseVal' is the correct column name\ntarget = california_df['MedHouseVal']\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n```\n:::\n\n\n# Ridge Regression: \nRidge regression, also known as Tikhonov regularization, introduces an L2 regularization term to the cost function. This term penalizes large coefficients, helping to mitigate multicollinearity and produce a more robust model.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Ridge Regression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nridge_model = Ridge(alpha=1.0)\nridge_model.fit(X_train, y_train)\nridge_y_pred = ridge_model.predict(X_test)\nridge_mse = mean_squared_error(y_test, ridge_y_pred)\nridge_r2 = r2_score(y_test, ridge_y_pred)\n```\n:::\n\n\n# Lasso Regression: \nLasso regression, or Least Absolute Shrinkage and Selection Operator, uses an L1 regularization term. In addition to penalizing large coefficients, Lasso performs feature selection by pushing some coefficients to exactly zero.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Lasso Regression\nfrom sklearn.linear_model import Lasso\n\nlasso_model = Lasso(alpha=1.0)\nlasso_model.fit(X_train, y_train)\nlasso_y_pred = lasso_model.predict(X_test)\nlasso_mse = mean_squared_error(y_test, lasso_y_pred)\nlasso_r2 = r2_score(y_test, lasso_y_pred)\n```\n:::\n\n\n# Visualization:\nVisualizing the predictions allows us to gain insights into how well the models perform. Scatter plots of actual vs. predicted values reveal patterns and errors in the predictions.\n\nNow, let's compare the performance metrics to understand how well each model generalizes to new data.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Visualize predictions\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, ridge_y_pred, alpha=0.7)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Ridge Regression Predictions')\n\nplt.subplot(1, 2, 2)\nplt.scatter(y_test, lasso_y_pred, alpha=0.7)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Lasso Regression Predictions')\n\nplt.tight_layout()\nplt.show()\n\n# Print results\nprint(\"Ridge Regression Results:\")\nprint(f'Mean Squared Error: {ridge_mse}')\nprint(f'R-squared: {ridge_r2}')\n\nprint(\"\\nLasso Regression Results:\")\nprint(f'Mean Squared Error: {lasso_mse}')\nprint(f'R-squared: {lasso_r2}')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=1141 height=564}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRidge Regression Results:\nMean Squared Error: 0.5558034669932213\nR-squared: 0.5758549611440125\n\nLasso Regression Results:\nMean Squared Error: 0.9380337514945428\nR-squared: 0.2841671821008396\n```\n:::\n:::\n\n\nIn comparing the results, we observe that both Ridge and Lasso regression provide effective regularization, reducing overfitting and improving model generalization. However, there are key differences:\n\nRidge Regression:\n\na. Penalizes large coefficients.\n\nb. Mitigates multicollinearity.\n\nc. Retains all features with reduced weights.\n\nLasso Regression:\n\na. Performs feature selection by pushing some coefficients to zero.\n\nb. Useful when there are a large number of irrelevant features.\n\nc. Results in a sparse model with fewer features.\n\nThe choice between Ridge and Lasso depends on the specific characteristics of the dataset and modeling goals. If feature interpretability is crucial, Lasso might be preferred for its feature selection capabilities. On the other hand, Ridge regression might be more suitable when all features contribute significantly to the target variable.\n\n# Non Linear Regression:\n\nNonlinear regression is a form of regression analysis in which the relationship between the dependent variable and the independent variables is modeled as a nonlinear function. Unlike linear regression, which assumes a linear relationship, nonlinear regression allows for more complex relationships between variables. The general form of a nonlinear regression model is: y=f(x,β)+ε\n\nThe key steps in performing nonlinear regression are similar to those in linear regression, but the main difference lies in estimating the parameters of the nonlinear function.\n\nThe California Housing dataset is a rich source of information for housing-related predictions. Bursting with features like median income, housing median age, and average rooms, it serves as an ideal playground for our exploration. Our focal point is predicting median house values, a task crucial for understanding real estate dynamics.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n## Load the California Housing dataset from scikit-learn\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n\ncalifornia_housing = fetch_california_housing(as_frame=True)\ncalifornia_df = california_housing.frame\n# Select one feature (median income) and the target variable\nfeatures = california_df[['MedInc']]\ntarget = california_df['MedHouseVal']\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n```\n:::\n\n\nTo capture more complex relationships, we turn to polynomial regression. This technique introduces polynomial terms, allowing our model to capture non-linear patterns. In our case, let's consider polynomial features of degree 2.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Apply polynomial features\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nX_poly_train=X_train.copy()\nX_poly_test=X_test.copy()\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly_train = poly_features.fit_transform(X_poly_train)\nX_poly_test = poly_features.transform(X_poly_test)\n\n# Standardize features\nscaler = StandardScaler()\nX_poly_train_scaled = scaler.fit_transform(X_poly_train)\nX_poly_test_scaled = scaler.transform(X_poly_test)\n\n```\n:::\n\n\nRidge Regression: Balancing Flexibility and Regularization.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Ridge Regression on Polynomial Features\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nridge_model = Ridge(alpha=1.0)\nridge_model.fit(X_poly_train_scaled, y_train)\nridge_y_pred = ridge_model.predict(X_poly_test_scaled)\nridge_mse = mean_squared_error(y_test, ridge_y_pred)\nridge_r2 = r2_score(y_test, ridge_y_pred)\n```\n:::\n\n\nLasso Regression: Embracing Sparsity in Features\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Lasso Regression on Polynomial Features\nfrom sklearn.linear_model import Lasso\n\nlasso_model = Lasso(alpha=0.01)\nlasso_model.fit(X_poly_train_scaled, y_train)\nlasso_y_pred = lasso_model.predict(X_poly_test_scaled)\nlasso_mse = mean_squared_error(y_test, lasso_y_pred)\nlasso_r2 = r2_score(y_test, lasso_y_pred)\n```\n:::\n\n\nVisualizing our predictions is crucial for understanding how well our models perform. Let's plot the actual data against the predictions made by Ridge and Lasso models.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Visualize predictions\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X_test, y_test, color='black', label='Actual Data')\nplt.scatter(X_test, ridge_y_pred, color='red', label='Ridge Predictions')\nplt.title('Ridge Regression on Polynomial Features')\nplt.xlabel('Median Income')\nplt.ylabel('Median House Value')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_test, y_test, color='black', label='Actual Data')\nplt.scatter(X_test, lasso_y_pred, color='blue', label='Lasso Predictions')\nplt.title('Lasso Regression on Polynomial Features')\nplt.xlabel('Median Income')\nplt.ylabel('Median House Value')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=1141 height=564}\n:::\n:::\n\n\nAs we conclude our journey, it's essential to assess the performance of our models. The metrics such as Mean Squared Error and R-squared provide insights into the accuracy and explanatory power of our models.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Print results\nprint(\"Ridge Regression Results:\")\nprint(f'Mean Squared Error: {ridge_mse}')\nprint(f'R-squared: {ridge_r2}')\n\nprint(\"\\nLasso Regression Results:\")\nprint(f'Mean Squared Error: {lasso_mse}')\nprint(f'R-squared: {lasso_r2}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRidge Regression Results:\nMean Squared Error: 0.7032715675121511\nR-squared: 0.4633190254417924\n\nLasso Regression Results:\nMean Squared Error: 0.7062259069529382\nR-squared: 0.46106450834839385\n```\n:::\n:::\n\n\nPolynomial regression with Ridge and Lasso regularization on the California Housing dataset has unveiled the intricate patterns hidden within the data. As we navigate the landscapes of non-linear regression, we discover the delicate balance between model flexibility and regularization.\n\nReferences have been taken from various sourc es on the Internet.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}