{
  "hash": "c504417e23b9a27d7ba3e2050df588ef",
  "result": {
    "markdown": "---\ntitle: \"Classification Blog\"\nauthor: \"Trisha Bora\"\ndate: \"2023-12-06\"\ncategories: [Classification]\nimage: \"image.jpg\"\n\n---\n\n# Title: Classification\n\n# Introduction: \nIn this blog, we will unravel the essentials of classification, the requirements for building machine learning code, and showcase the beauty of data visualization.\n\n# Understanding Classification:\nClassification is a type of supervised learning where the algorithm learns from labeled training data to categorize new, unseen data into predefined classes or categories. For instance teaching a computer to distinguish between spam and non-spam emails or identify the genre of a song based on its features.\n\n# Requirements for Machine Learning Code:\n\nData Collection and Preprocessing: A dataset that represents the problem is needed to be solved has to be collected. Then the data needs to be cleaned and preprocess the data to handle missing values, outliers, and standardize features.\n\nSplitting Data: The dataset is divided into training and testing sets. The training set is used to train the model, while the testing set evaluates its performance on unseen data.\n\nChoosing a Model: A suitable classification algorithm based on the nature of the problem is needed to be selected. Common algorithms include Decision Trees, Support Vector Machines, Logistic Regression, and Neural Networks.\n\nTraining the Model: The training data is fed into the chosen algorithm and let it learn the patterns and relationships between features and labels. Model Evaluation: The model's performance is assessed using metrics like accuracy, precision, recall, and F1 score on the testing set. Fine-Tuning: The model parameters are optimized to improve its performance. Techniques like cross-validation can help in this phase. Deployment: Once satisfied with the model's performance, it is deployed for making predictions on new, unseen data.\n\nData Visualization in Classification: Now, let's bring the power of data visualization into play with a simple example.\n\nConsider a binary classification problem where we aim to classify whether a student passes or fails based on two features:\n\n1) hours of study\n\n2) previous exam score.\n\nIn this example, we use a scatter plot to visualize the data points, with different colors representing pass and fail. Additionally, we visualize the confusion matrix to evaluate the model's performance.\n\n# The Wine Dataset:\nA Palette of Three Classes The Wine dataset serves as the canvas for our exploration. Comprising 13 features related to chemical analyses of wines grown in a specific Italian region. For the sake of visualization, we have chosen to focus on the first two features, standardizing them for optimal clarity.\n\nData Preprocessing\n\nFeature Selection: Limiting the analysis to the first two features for visualization purposes.\n\nStandardization: Scaling the features to a common range for improved visualization.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the Wine dataset\nwine = load_wine()\nX = wine.data[:, :2]  # Use only the first two features for visualization\ny = wine.target\n# Standardize the features for better visualization of decision boundary\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n```\n:::\n\n\nAt the heart of classification algorithms lies KNN, a method both simplistic and powerful. The essence of KNN lies in its ability to make predictions based on the majority class among the k-nearest neighbors. It is a non-parametric, lazy learning algorithm, meaning it makes predictions at runtime rather than during training.\n\n# How KNN Works:\n\nThe operational paradigm of KNN involves the following steps:\n\nDistance Computation: Calculate the distance between a query point and all other points in the dataset. Nearest Neighbors Selection:\nIdentify the k-nearest neighbors to the query point based on calculated distances.\n\nMajority Voting: Assign the class label to the query point based on the majority class among its k-nearest neighbors. KNNâ€™s simplicity makes it an ideal candidate for our exploration into the intricacies of decision boundaries.\n\nThe code used to train the above data set and how it has been visualised is shown below.\n\nThe code showcases the class for each point on the data set is determined and visualize the decision boundaries.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.2, random_state=42)\n\n# Create a k-Nearest Neighbors (k-NN) classifier\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Train the classifier on the training data\nknn.fit(X_train, y_train)\n\n# Make predictions on the test data\npredictions = knn.predict(X_test)\n\n# Evaluate the accuracy of the model\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Plot the decision boundary with feature names\nh = .02  # Step size in the mesh\nx_min, x_max = X_std[:, 0].min() - 1, X_std[:, 0].max() + 1\ny_min, y_max = X_std[:, 1].min() - 1, X_std[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Create a meshgrid of points and predict the class for each point\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(10, 8))\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n\n# Plot the training points\nscatter = plt.scatter(X_std[:, 0], X_std[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\nplt.xlabel(f'{wine.feature_names[0]} (Standardized)')\nplt.ylabel(f'{wine.feature_names[1]} (Standardized)')\n\n# Add legend with target names\nplt.legend(*scatter.legend_elements(), title='Wine Classes')\n\n# Display feature names near the points\nfor i, txt in enumerate(wine.feature_names[:2]):\n    plt.annotate(txt, (X_std[i, 0], X_std[i, 1]), fontsize=8, color='black')\n\nplt.title('Decision Boundary for KNeighborsClassifier (n_neighbors=3) on Wine Dataset')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.89\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=811 height=660}\n:::\n:::\n\n\n# Interpreting the Visuals:\nAs we gaze upon the mesmerizing plot, each region's color unveils the model's decision boundary, distinguishing the wine classes. The scattered points represent the training data, each one contributing to the algorithm's understanding of the dataset.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n# Plot the confusion matrix\nconf_matrix = confusion_matrix(y_test, predictions)\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=wine.target_names, yticklabels=wine.target_names)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=614 height=515}\n:::\n:::\n\n\n# Interpreting the Confusion Matrix:\n\nThe confusion matrix provides a granular view of the model's performance by breaking down predictions into four categories:\n\nTrue Positives (TP): Instances where the model correctly predicts the positive class.\n\nTrue Negatives (TN): Instances where the model correctly predicts the negative class.\n\nFalse Positives (FP): Instances where the model incorrectly predicts the positive class.\n\nFalse Negatives (FN): Instances where the model incorrectly predicts the negative class.\n\nThe provided code generates a heatmap to visualize the confusion matrix for a multiclass classification task using the KNeighborsClassifier with 3 neighbors. The confusion matrix summarizes the model's performance by comparing predicted and true class labels. Each cell in the heatmap represents the count of instances classified into specific categories. The annotations within the cells provide a quantitative measure of the model's accuracy. The color intensity, ranging from light to dark blue, conveys the magnitude of correct and incorrect predictions. This visualization aids in identifying which classes the model excels at predicting and where it may encounter challenges, offering a comprehensive and intuitive overview of the classification performance on the Wine dataset.\n\n\nReferences have been taken from various sources on the Internet.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}