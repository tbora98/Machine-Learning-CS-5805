{
  "hash": "b04b5a9b1b43148ce4a8c1cacaf0ee08",
  "result": {
    "markdown": "---\ntitle: \"Anomaly Detection Blog\"\nauthor: \"Trisha Bora\"\ndate: \"2023-12-06\"\ncategories: [ Anomaly Detection]\nimage: \"image.jpg\"\n\n---\n\n# Title: Anomaly Detection:\nAnomaly detection, a critical facet across various domains, serves as a robust mechanism for identifying irregular patterns or data points. Its fundamental principle involves establishing a norm and categorizing deviations as anomalies. These anomalies can manifest as point anomalies, contextual anomalies linked to specific conditions, or collective anomalies within a set of related data points. Techniques for anomaly detection span statistical methods like Z-score and Gaussian distribution, machine learning approaches such as Isolation Forest and Autoencoders, and the utilization of deep learning neural networks.\n# Data Preprocessing\nThe Wine dataset, a classic dataset often used for machine learning exploration, consists of various chemical attributes of wines. Leveraging Isolation Forest, a tree-based anomaly detection algorithm, we aim to highlight instances that deviate significantly from the norm. To get started, we load the Wine dataset and preprocess the data. Standardization and dimensionality reduction using Principal Component Analysis (PCA) set the stage for more effective anomaly detection.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_wine\n\n# Load the Wine dataset\nwine = load_wine()\nX = wine.data\ny = wine.target\n\n# Standardize the data\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n\n# Apply PCA for dimensionality reduction\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_std)\n```\n:::\n\n\n# Isolation Forest:\nIsolation Forest works by isolating anomalies through recursive partitioning. The algorithm assigns anomaly scores to each data point, representing their likelihood of being an outlier. We fit the Isolation Forest model and obtain anomaly scores for the Wine dataset.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Fit the Isolation Forest model\nmodel = IsolationForest(contamination=0.05, random_state=42)\ny_pred = model.fit_predict(X_std)\nanomaly_scores = model.decision_function(X_std)\n```\n:::\n\n\n# Visualization:\nVisualization is key to understanding the algorithm's findings. In our case, we use a scatter plot with the first principal component on the x-axis, anomaly scores on the y-axis, and connections between points. Anomalies are highlighted in red.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Create a scatter plot with lines connecting the points\nplt.figure(figsize=(12, 6))\n\n# Plot the values along the first principal component\nplt.scatter(range(len(X_pca)), X_pca[:, 0], label='Data', c=anomaly_scores, cmap='viridis', alpha=0.7)\n\n# Connect the points with lines\nplt.plot(range(len(X_pca)), X_pca[:, 0], linestyle='-', color='blue', alpha=0.5)\n\n# Highlight anomalies with red circles\nplt.scatter(np.where(y_pred == -1), X_pca[y_pred == -1, 0], c='red', marker='o', label='Anomaly')\n\nplt.title('Isolation Forest Anomaly Detection on Wine Dataset with Anomaly Scores')\nplt.xlabel('Index')\nplt.ylabel('Principal Component 2')\nplt.colorbar(label='Anomaly Score')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=896 height=515}\n:::\n:::\n\n\nThe resulting plot offers a visual representation of anomalies detected by the Isolation Forest algorithm. Instances with higher anomaly scores, depicted in red, stand out as potential outliers. Adjusting the threshold for anomaly scores allows for fine-tuning the sensitivity of the outlier detection. In this exploration, we applied Isolation Forest to the Wine dataset, shedding light on potential outliers based on chemical attributes. The combination of dimensionality reduction, anomaly scoring, and visualization provides a comprehensive view of the algorithm's performance.\n\n# Local Outlier Factor (LOF) algorithm:\nAnother popular method for anomaly detection is the Local Outlier Factor (LOF) algorithm. LOF assesses the local density deviation of data points, identifying those with significantly lower density as potential outliers. It's effective in scenarios where anomalies might not necessarily be distant from the majority but exhibit lower density.\n\nUnderstanding Local Outlier Factor (LOF): The LOF algorithm assigns an anomaly score to each data point based on the local density comparison with its neighbors. Points with lower density compared to their neighbors receive higher anomaly scores, marking them as potential outliers. \n\nThe following visualization is used to demonstrate this technique for detecting anomalies for one of the features among 13 features present in the wine dataset-\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_wine\nfrom sklearn.neighbors import LocalOutlierFactor\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the wine dataset\nwine = load_wine()\nX = wine.data\n\n# Use Local Outlier Factor for anomaly detection\nmodel = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\npredictions = model.fit_predict(X)\n\n# Visualize scatter plot for feature 2 and histogram for feature 3\nfeature2_index = 2\nfeature3_index = 3\n\nplt.figure(figsize=(12, 6))\n\n\n# Histogram for feature 3\nplt.subplot(1, 2, 2)\nplt.hist(X[predictions == 1, feature3_index], bins=30, color='green', alpha=0.5, label='Normal')\nplt.hist(X[predictions == -1, feature3_index], bins=30, color='red', alpha=0.5, label='Anomalies')\nplt.title(f'Histogram for Feature {wine.feature_names[feature3_index]}')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=593 height=565}\n:::\n:::\n\n\nReferences have been taken from various sources on the Internet.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}