[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "About this blog\nThis is my first time posting in a Quarto blog. This has been created as the assignment of the course offered in Virginia Tech, Machine Learning I, CS 5805. Welcome!\nI am Trisha Bora, a graduate student in Virginia tech blacsburg campus with my focus in Electrical Engineering.\nHope you like the blogs!"
  },
  {
    "objectID": "posts/Linear-and-NonLinear-Regression/index.html",
    "href": "posts/Linear-and-NonLinear-Regression/index.html",
    "title": "Linear and Non Linear Regression Blog",
    "section": "",
    "text": "Title: Linear and Non Linear Regression.\n\n\nLinear regression:\nLinear regression stands as the cornerstone of predictive modeling. It’s a method that seeks to establish a linear relationship between a dependent variable and one or more independent variables. This relationship is expressed through a linear equation, allowing us to make predictions based on the observed data. Performing linear regression involves several steps. Here’s a general outline of the process:\nCollect Data: A dataset with paired observations for the dependent and independent variables is desired.\nExplore the Data: We need to understand the characteristics of the collected data. This may involve plotting the data, checking for outliers, and examining the distribution of variables.\nDefine Variables: Identify the dependent variable and the independent variables.\nSplit the Data: Divide your dataset into two subsets: one for training the model and one for testing the model. The training set is used to fit the model, and the testing set is used to evaluate its performance.\nBuild the Model: For simple linear regression (one independent variable), the model is represented as: y=mx+b. Use statistical methods or optimization algorithms (such as the least squares method) to find the coefficients that minimize the difference between predicted and actual values in the training set.\nEvaluate the Model: Use the testing set to assess the model’s performance. Common evaluation metrics include mean squared error (MSE), root mean squared error (RMSE), and R-squared.\nInterpret the Results: Analyze the coefficients to understand the relationship between the variables. A positive coefficient indicates a positive relationship, and a negative coefficient indicates a negative relationship.\nMake Predictions: Once the model’s performance is satisfactory, predictions can be made on new data.\nValidate Assumptions: Check whether the assumptions of linear regression are met. These assumptions include linearity, independence of errors, homoscedasticity (constant variance of errors), and normality of errors.\nMachine Learning Code for Linear Regression: Let’s dive into the practical aspect with a Python script using the ubiquitous scikit-learn library. The California housing dataset is condidered.\nThis code crafts a narrative of linear regression using the California housing dataset, illuminating the relationship between selected features and house prices.\nLinear regression is a powerful tool for predicting numerical values, but when dealing with datasets that have a large number of features, the risk of overfitting becomes significant. In such cases, regularization techniques like Ridge and Lasso Regression come to the rescue by introducing penalty terms to the cost function. In this blog post, we’ll delve into the California Housing dataset and compare the performance of Ridge and Lasso Regression.\nThe California Housing Dataset:\nThe California Housing dataset is a widely used dataset for regression tasks. It contains housing data for districts in California, including features such as median income, housing median age, average rooms, etc. Our goal is to predict the median house value for California districts.\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n# Load the California Housing dataset from scikit-learn\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n\ncalifornia_housing = fetch_california_housing(as_frame=True)\ncalifornia_df = california_housing.frame\n\n# Select features and target\nfeatures = california_df.drop('MedHouseVal', axis=1)  # 'MedHouseVal' is the correct column name\ntarget = california_df['MedHouseVal']\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\n\n\nRidge Regression:\nRidge regression, also known as Tikhonov regularization, introduces an L2 regularization term to the cost function. This term penalizes large coefficients, helping to mitigate multicollinearity and produce a more robust model.\n\n# Ridge Regression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nridge_model = Ridge(alpha=1.0)\nridge_model.fit(X_train, y_train)\nridge_y_pred = ridge_model.predict(X_test)\nridge_mse = mean_squared_error(y_test, ridge_y_pred)\nridge_r2 = r2_score(y_test, ridge_y_pred)\n\n\n\nLasso Regression:\nLasso regression, or Least Absolute Shrinkage and Selection Operator, uses an L1 regularization term. In addition to penalizing large coefficients, Lasso performs feature selection by pushing some coefficients to exactly zero.\n\n# Lasso Regression\nfrom sklearn.linear_model import Lasso\n\nlasso_model = Lasso(alpha=1.0)\nlasso_model.fit(X_train, y_train)\nlasso_y_pred = lasso_model.predict(X_test)\nlasso_mse = mean_squared_error(y_test, lasso_y_pred)\nlasso_r2 = r2_score(y_test, lasso_y_pred)\n\n\n\nVisualization:\nVisualizing the predictions allows us to gain insights into how well the models perform. Scatter plots of actual vs. predicted values reveal patterns and errors in the predictions.\nNow, let’s compare the performance metrics to understand how well each model generalizes to new data.\n\n# Visualize predictions\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, ridge_y_pred, alpha=0.7)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Ridge Regression Predictions')\n\nplt.subplot(1, 2, 2)\nplt.scatter(y_test, lasso_y_pred, alpha=0.7)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Lasso Regression Predictions')\n\nplt.tight_layout()\nplt.show()\n\n# Print results\nprint(\"Ridge Regression Results:\")\nprint(f'Mean Squared Error: {ridge_mse}')\nprint(f'R-squared: {ridge_r2}')\n\nprint(\"\\nLasso Regression Results:\")\nprint(f'Mean Squared Error: {lasso_mse}')\nprint(f'R-squared: {lasso_r2}')\n\n\n\n\nRidge Regression Results:\nMean Squared Error: 0.5558034669932213\nR-squared: 0.5758549611440125\n\nLasso Regression Results:\nMean Squared Error: 0.9380337514945428\nR-squared: 0.2841671821008396\n\n\nIn comparing the results, we observe that both Ridge and Lasso regression provide effective regularization, reducing overfitting and improving model generalization. However, there are key differences:\nRidge Regression:\n\nPenalizes large coefficients.\nMitigates multicollinearity.\nRetains all features with reduced weights.\n\nLasso Regression:\n\nPerforms feature selection by pushing some coefficients to zero.\nUseful when there are a large number of irrelevant features.\nResults in a sparse model with fewer features.\n\nThe choice between Ridge and Lasso depends on the specific characteristics of the dataset and modeling goals. If feature interpretability is crucial, Lasso might be preferred for its feature selection capabilities. On the other hand, Ridge regression might be more suitable when all features contribute significantly to the target variable.\n\n\nNon Linear Regression:\nNonlinear regression is a form of regression analysis in which the relationship between the dependent variable and the independent variables is modeled as a nonlinear function. Unlike linear regression, which assumes a linear relationship, nonlinear regression allows for more complex relationships between variables. The general form of a nonlinear regression model is: y=f(x,β)+ε\nThe key steps in performing nonlinear regression are similar to those in linear regression, but the main difference lies in estimating the parameters of the nonlinear function.\nThe California Housing dataset is a rich source of information for housing-related predictions. Bursting with features like median income, housing median age, and average rooms, it serves as an ideal playground for our exploration. Our focal point is predicting median house values, a task crucial for understanding real estate dynamics.\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n## Load the California Housing dataset from scikit-learn\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n\ncalifornia_housing = fetch_california_housing(as_frame=True)\ncalifornia_df = california_housing.frame\n# Select one feature (median income) and the target variable\nfeatures = california_df[['MedInc']]\ntarget = california_df['MedHouseVal']\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\nTo capture more complex relationships, we turn to polynomial regression. This technique introduces polynomial terms, allowing our model to capture non-linear patterns. In our case, let’s consider polynomial features of degree 2.\n\n# Apply polynomial features\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nX_poly_train=X_train.copy()\nX_poly_test=X_test.copy()\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly_train = poly_features.fit_transform(X_poly_train)\nX_poly_test = poly_features.transform(X_poly_test)\n\n# Standardize features\nscaler = StandardScaler()\nX_poly_train_scaled = scaler.fit_transform(X_poly_train)\nX_poly_test_scaled = scaler.transform(X_poly_test)\n\nRidge Regression: Balancing Flexibility and Regularization.\n\n# Ridge Regression on Polynomial Features\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nridge_model = Ridge(alpha=1.0)\nridge_model.fit(X_poly_train_scaled, y_train)\nridge_y_pred = ridge_model.predict(X_poly_test_scaled)\nridge_mse = mean_squared_error(y_test, ridge_y_pred)\nridge_r2 = r2_score(y_test, ridge_y_pred)\n\nLasso Regression: Embracing Sparsity in Features\n\n# Lasso Regression on Polynomial Features\nfrom sklearn.linear_model import Lasso\n\nlasso_model = Lasso(alpha=0.01)\nlasso_model.fit(X_poly_train_scaled, y_train)\nlasso_y_pred = lasso_model.predict(X_poly_test_scaled)\nlasso_mse = mean_squared_error(y_test, lasso_y_pred)\nlasso_r2 = r2_score(y_test, lasso_y_pred)\n\nVisualizing our predictions is crucial for understanding how well our models perform. Let’s plot the actual data against the predictions made by Ridge and Lasso models.\n\n# Visualize predictions\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X_test, y_test, color='black', label='Actual Data')\nplt.scatter(X_test, ridge_y_pred, color='red', label='Ridge Predictions')\nplt.title('Ridge Regression on Polynomial Features')\nplt.xlabel('Median Income')\nplt.ylabel('Median House Value')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_test, y_test, color='black', label='Actual Data')\nplt.scatter(X_test, lasso_y_pred, color='blue', label='Lasso Predictions')\nplt.title('Lasso Regression on Polynomial Features')\nplt.xlabel('Median Income')\nplt.ylabel('Median House Value')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nAs we conclude our journey, it’s essential to assess the performance of our models. The metrics such as Mean Squared Error and R-squared provide insights into the accuracy and explanatory power of our models.\n\n# Print results\nprint(\"Ridge Regression Results:\")\nprint(f'Mean Squared Error: {ridge_mse}')\nprint(f'R-squared: {ridge_r2}')\n\nprint(\"\\nLasso Regression Results:\")\nprint(f'Mean Squared Error: {lasso_mse}')\nprint(f'R-squared: {lasso_r2}')\n\nRidge Regression Results:\nMean Squared Error: 0.7032715675121511\nR-squared: 0.4633190254417924\n\nLasso Regression Results:\nMean Squared Error: 0.7062259069529382\nR-squared: 0.46106450834839385\n\n\nPolynomial regression with Ridge and Lasso regularization on the California Housing dataset has unveiled the intricate patterns hidden within the data. As we navigate the landscapes of non-linear regression, we discover the delicate balance between model flexibility and regularization.\nReferences have been taken from various sources on the Internet."
  },
  {
    "objectID": "posts/Probablity-and-random-variables/index.html",
    "href": "posts/Probablity-and-random-variables/index.html",
    "title": "Probability Theory and Random Variables Blog",
    "section": "",
    "text": "Title: Probability Theory and Random Variables.\n\n\nUnderstanding Probability:\nProbability theory is the mathematical framework for dealing with uncertainty. In machine learning, where decisions are often made based on incomplete or noisy data, understanding probability is paramount. At its core, probability theory allows us to quantify uncertainty and make informed decisions.\n\n\nRandom Variables:\nRandom variables are a key concept in probability theory. They represent quantities that can take on different values, each with a certain probability. In the context of machine learning, random variables are used to model the uncertainty inherent in data and predictions.\nConsider a scenario where we are predicting the outcome of a coin toss. The result, whether heads or tails, is a random variable with equal probabilities. In machine learning, we often deal with more complex random variables representing features of data or the outcomes of various events.\n\n\nProbability Distributions:\nA probability distribution describes the likelihood of different outcomes in a random experiment. Common probability distributions include the uniform, normal (Gaussian), and binomial distributions. In machine learning, understanding the distribution of data is crucial for building accurate models.\n\n\nRole in Machine Learning Code:\nLet’s explore how probability theory and random variables manifest in machine learning code.\nIn classification problems, for instance, we use probability distributions to estimate the likelihood of a data point belonging to a particular class. This is often done through algorithms like Naive Bayes or logistic regression.\nProbability theory and random variables serve as the bedrock of insights in machine learning, unveiling patterns and relationships hidden within datasets. In this journey, we’ll explore these concepts using scikit-learn’s Wine dataset, showcasing practical demonstrations that demystify complex statistical concepts.\n\n\nUnveiling Normality:\nZ-Scores and Alcohol Content Let’s embark on a journey of normality using the Wine dataset. Selecting the alcohol content as our protagonist, we delve into its distribution. Visualizing this through a histogram, we witness the elegance of probability distributions. Calculating Z-scores adds a layer of statistical prowess, unraveling the probability density of each data point.\n\nfrom sklearn.datasets import load_wine\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\nwine = load_wine()\nalcohol_content = wine.data[:, 0]\n\n# Plot histogram\nplt.hist(alcohol_content, bins=20, density=True, alpha=0.7, color='blue')\n\n# Calculate Z-scores\nz_scores = zscore(alcohol_content)\n\nplt.title(\"Distribution of Alcohol Content\")\nplt.xlabel(\"Alcohol Content\")\nplt.ylabel(\"Probability Density\")\nplt.show()\n\n\n\n\n\n\nConditional Harmony:\nFlavanoids and Color Intensity Next, we navigate the realm of conditional probability with a captivating scatter plot. The protagonists, flavanoids, and color intensity, engage in a visual dance, revealing how changes in one feature influence the probability distribution of another. This demonstration bridges the abstract with the tangible, making conditional probability an intuitive concept.\n\nfrom sklearn.datasets import load_wine\nimport matplotlib.pyplot as plt\n\nwine = load_wine()\nflavanoids = wine.data[:, 6]  # Flavanoids\ncolor_intensity = wine.data[:, 9]  # Color intensity\n\n# Scatter plot\nplt.scatter(flavanoids, color_intensity, alpha=0.7)\nplt.title(\"Scatter Plot: Flavanoids vs Color Intensity\")\nplt.xlabel(\"Flavanoids\")\nplt.ylabel(\"Color Intensity\")\nplt.show()\n\n\n\n\n\n\nHarmonizing with the Central Limit Theorem:\nTotal Phenols Symphony As we shift our focus to the central limit theorem, we choose total phenols as our musical notes. Through a symphony of sample means, we showcase how the distribution converges to a normality as the sample size increases. This exploration sheds light on the inherent beauty of statistical regularities within diverse datasets.\n\nfrom sklearn.datasets import load_wine\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nwine = load_wine()\ntotal_phenols = wine.data[:, 8]  # Total phenols\n\nsample_means = [np.mean(np.random.choice(total_phenols, size=30)) for _ in range(1000)]\n\n# Plot distribution of sample means\nplt.hist(sample_means, bins=30, density=True, alpha=0.7, color='green')\n\nplt.title(\"Distribution of Sample Means (Total Phenols)\")\nplt.xlabel(\"Sample Means\")\nplt.ylabel(\"Probability Density\")\nplt.show()\n\n\n\n\n\n\nConclusion:\nProbability theory and random variables aren’t mere theoretical constructs but guiding stars illuminating the path to machine learning insights. Through the lens of the Wine dataset, we’ve navigated the seas of normality, conditional probability, and the harmonious central limit theorem. Armed with this understanding, machine learning practitioners can craft more informed models and make data-driven decisions.\nReferences have been taken from various sources on the Internet."
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification Blog",
    "section": "",
    "text": "Title: Classification\n\n\nIntroduction:\nIn this blog, we will unravel the essentials of classification, the requirements for building machine learning code, and showcase the beauty of data visualization.\n\n\nUnderstanding Classification:\nClassification is a type of supervised learning where the algorithm learns from labeled training data to categorize new, unseen data into predefined classes or categories. For instance teaching a computer to distinguish between spam and non-spam emails or identify the genre of a song based on its features.\n\n\nRequirements for Machine Learning Code:\nData Collection and Preprocessing: A dataset that represents the problem is needed to be solved has to be collected. Then the data needs to be cleaned and preprocess the data to handle missing values, outliers, and standardize features.\nSplitting Data: The dataset is divided into training and testing sets. The training set is used to train the model, while the testing set evaluates its performance on unseen data.\nChoosing a Model: A suitable classification algorithm based on the nature of the problem is needed to be selected. Common algorithms include Decision Trees, Support Vector Machines, Logistic Regression, and Neural Networks.\nTraining the Model: The training data is fed into the chosen algorithm and let it learn the patterns and relationships between features and labels. Model Evaluation: The model’s performance is assessed using metrics like accuracy, precision, recall, and F1 score on the testing set. Fine-Tuning: The model parameters are optimized to improve its performance. Techniques like cross-validation can help in this phase. Deployment: Once satisfied with the model’s performance, it is deployed for making predictions on new, unseen data.\nData Visualization in Classification: Now, let’s bring the power of data visualization into play with a simple example.\nConsider a binary classification problem where we aim to classify whether a student passes or fails based on two features:\n\nhours of study\nprevious exam score.\n\nIn this example, we use a scatter plot to visualize the data points, with different colors representing pass and fail. Additionally, we visualize the confusion matrix to evaluate the model’s performance.\n\n\nThe Wine Dataset:\nA Palette of Three Classes The Wine dataset serves as the canvas for our exploration. Comprising 13 features related to chemical analyses of wines grown in a specific Italian region. For the sake of visualization, we have chosen to focus on the first two features, standardizing them for optimal clarity.\nData Preprocessing\nFeature Selection: Limiting the analysis to the first two features for visualization purposes.\nStandardization: Scaling the features to a common range for improved visualization.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the Wine dataset\nwine = load_wine()\nX = wine.data[:, :2]  # Use only the first two features for visualization\ny = wine.target\n# Standardize the features for better visualization of decision boundary\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n\nAt the heart of classification algorithms lies KNN, a method both simplistic and powerful. The essence of KNN lies in its ability to make predictions based on the majority class among the k-nearest neighbors. It is a non-parametric, lazy learning algorithm, meaning it makes predictions at runtime rather than during training.\n\n\nHow KNN Works:\nThe operational paradigm of KNN involves the following steps:\nDistance Computation: Calculate the distance between a query point and all other points in the dataset. Nearest Neighbors Selection: Identify the k-nearest neighbors to the query point based on calculated distances.\nMajority Voting: Assign the class label to the query point based on the majority class among its k-nearest neighbors. KNN’s simplicity makes it an ideal candidate for our exploration into the intricacies of decision boundaries.\nThe code used to train the above data set and how it has been visualised is shown below.\nThe code showcases the class for each point on the data set is determined and visualize the decision boundaries.\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.2, random_state=42)\n\n# Create a k-Nearest Neighbors (k-NN) classifier\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Train the classifier on the training data\nknn.fit(X_train, y_train)\n\n# Make predictions on the test data\npredictions = knn.predict(X_test)\n\n# Evaluate the accuracy of the model\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Plot the decision boundary with feature names\nh = .02  # Step size in the mesh\nx_min, x_max = X_std[:, 0].min() - 1, X_std[:, 0].max() + 1\ny_min, y_max = X_std[:, 1].min() - 1, X_std[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Create a meshgrid of points and predict the class for each point\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(10, 8))\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n\n# Plot the training points\nscatter = plt.scatter(X_std[:, 0], X_std[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\nplt.xlabel(f'{wine.feature_names[0]} (Standardized)')\nplt.ylabel(f'{wine.feature_names[1]} (Standardized)')\n\n# Add legend with target names\nplt.legend(*scatter.legend_elements(), title='Wine Classes')\n\n# Display feature names near the points\nfor i, txt in enumerate(wine.feature_names[:2]):\n    plt.annotate(txt, (X_std[i, 0], X_std[i, 1]), fontsize=8, color='black')\n\nplt.title('Decision Boundary for KNeighborsClassifier (n_neighbors=3) on Wine Dataset')\nplt.show()\n\nAccuracy: 0.89\n\n\n\n\n\n\n\nInterpreting the Visuals:\nAs we gaze upon the mesmerizing plot, each region’s color unveils the model’s decision boundary, distinguishing the wine classes. The scattered points represent the training data, each one contributing to the algorithm’s understanding of the dataset.\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n# Plot the confusion matrix\nconf_matrix = confusion_matrix(y_test, predictions)\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=wine.target_names, yticklabels=wine.target_names)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\n\n\n\n\n\nInterpreting the Confusion Matrix:\nThe confusion matrix provides a granular view of the model’s performance by breaking down predictions into four categories:\nTrue Positives (TP): Instances where the model correctly predicts the positive class.\nTrue Negatives (TN): Instances where the model correctly predicts the negative class.\nFalse Positives (FP): Instances where the model incorrectly predicts the positive class.\nFalse Negatives (FN): Instances where the model incorrectly predicts the negative class.\nThe provided code generates a heatmap to visualize the confusion matrix for a multiclass classification task using the KNeighborsClassifier with 3 neighbors. The confusion matrix summarizes the model’s performance by comparing predicted and true class labels. Each cell in the heatmap represents the count of instances classified into specific categories. The annotations within the cells provide a quantitative measure of the model’s accuracy. The color intensity, ranging from light to dark blue, conveys the magnitude of correct and incorrect predictions. This visualization aids in identifying which classes the model excels at predicting and where it may encounter challenges, offering a comprehensive and intuitive overview of the classification performance on the Wine dataset.\nReferences have been taken from various sources on the Internet."
  },
  {
    "objectID": "posts/Anomaly_Detection/index.html",
    "href": "posts/Anomaly_Detection/index.html",
    "title": "Anomaly Detection Blog",
    "section": "",
    "text": "Title: Anomaly Detection:\nAnomaly detection, a critical facet across various domains, serves as a robust mechanism for identifying irregular patterns or data points. Its fundamental principle involves establishing a norm and categorizing deviations as anomalies. These anomalies can manifest as point anomalies, contextual anomalies linked to specific conditions, or collective anomalies within a set of related data points. Techniques for anomaly detection span statistical methods like Z-score and Gaussian distribution, machine learning approaches such as Isolation Forest and Autoencoders, and the utilization of deep learning neural networks. # Data Preprocessing The Wine dataset, a classic dataset often used for machine learning exploration, consists of various chemical attributes of wines. Leveraging Isolation Forest, a tree-based anomaly detection algorithm, we aim to highlight instances that deviate significantly from the norm. To get started, we load the Wine dataset and preprocess the data. Standardization and dimensionality reduction using Principal Component Analysis (PCA) set the stage for more effective anomaly detection.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_wine\n\n# Load the Wine dataset\nwine = load_wine()\nX = wine.data\ny = wine.target\n\n# Standardize the data\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n\n# Apply PCA for dimensionality reduction\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_std)\n\n\n\nIsolation Forest:\nIsolation Forest works by isolating anomalies through recursive partitioning. The algorithm assigns anomaly scores to each data point, representing their likelihood of being an outlier. We fit the Isolation Forest model and obtain anomaly scores for the Wine dataset.\n\n# Fit the Isolation Forest model\nmodel = IsolationForest(contamination=0.05, random_state=42)\ny_pred = model.fit_predict(X_std)\nanomaly_scores = model.decision_function(X_std)\n\n\n\nVisualization:\nVisualization is key to understanding the algorithm’s findings. In our case, we use a scatter plot with the first principal component on the x-axis, anomaly scores on the y-axis, and connections between points. Anomalies are highlighted in red.\n\n# Create a scatter plot with lines connecting the points\nplt.figure(figsize=(12, 6))\n\n# Plot the values along the first principal component\nplt.scatter(range(len(X_pca)), X_pca[:, 0], label='Data', c=anomaly_scores, cmap='viridis', alpha=0.7)\n\n# Connect the points with lines\nplt.plot(range(len(X_pca)), X_pca[:, 0], linestyle='-', color='blue', alpha=0.5)\n\n# Highlight anomalies with red circles\nplt.scatter(np.where(y_pred == -1), X_pca[y_pred == -1, 0], c='red', marker='o', label='Anomaly')\n\nplt.title('Isolation Forest Anomaly Detection on Wine Dataset with Anomaly Scores')\nplt.xlabel('Index')\nplt.ylabel('Principal Component 2')\nplt.colorbar(label='Anomaly Score')\nplt.legend()\nplt.show()\n\n\n\n\nThe resulting plot offers a visual representation of anomalies detected by the Isolation Forest algorithm. Instances with higher anomaly scores, depicted in red, stand out as potential outliers. Adjusting the threshold for anomaly scores allows for fine-tuning the sensitivity of the outlier detection. In this exploration, we applied Isolation Forest to the Wine dataset, shedding light on potential outliers based on chemical attributes. The combination of dimensionality reduction, anomaly scoring, and visualization provides a comprehensive view of the algorithm’s performance.\n\n\nLocal Outlier Factor (LOF) algorithm:\nAnother popular method for anomaly detection is the Local Outlier Factor (LOF) algorithm. LOF assesses the local density deviation of data points, identifying those with significantly lower density as potential outliers. It’s effective in scenarios where anomalies might not necessarily be distant from the majority but exhibit lower density.\nUnderstanding Local Outlier Factor (LOF): The LOF algorithm assigns an anomaly score to each data point based on the local density comparison with its neighbors. Points with lower density compared to their neighbors receive higher anomaly scores, marking them as potential outliers.\nThe following visualization is used to demonstrate this technique for detecting anomalies for one of the features among 13 features present in the wine dataset-\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.neighbors import LocalOutlierFactor\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the wine dataset\nwine = load_wine()\nX = wine.data\n\n# Use Local Outlier Factor for anomaly detection\nmodel = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\npredictions = model.fit_predict(X)\n\n# Visualize scatter plot for feature 2 and histogram for feature 3\nfeature2_index = 2\nfeature3_index = 3\n\nplt.figure(figsize=(12, 6))\n\n\n# Histogram for feature 3\nplt.subplot(1, 2, 2)\nplt.hist(X[predictions == 1, feature3_index], bins=30, color='green', alpha=0.5, label='Normal')\nplt.hist(X[predictions == -1, feature3_index], bins=30, color='red', alpha=0.5, label='Anomalies')\nplt.title(f'Histogram for Feature {wine.feature_names[feature3_index]}')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nReferences have been taken from various sources on the Internet."
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering Blog",
    "section": "",
    "text": "Title: Clustering\nClustering is a powerful technique that allows us to uncover hidden patterns and structures within data.\nIn this blog post, the fundamentals of clustering, the requirements for implementing machine learning code, and showcase a data visualization to better understand the concept behind clustering algorithms.\n\n\nUnderstanding Clustering:\nClustering is a type of unsupervised learning where the goal is to group similar data points together. Unlike supervised learning, there are no predefined labels for the data, and the algorithm must identify patterns on its own. This makes clustering particularly useful for exploratory data analysis, customer segmentation, anomaly detection, and more. Requirements for Machine Learning Code:\nSelecting a Clustering Algorithm: There are various clustering algorithms available, each with its strengths and weaknesses. Common algorithms include K-Means, Hierarchical Clustering, and DBSCAN. The choice depends on the nature of the data and the desired outcome.\nData Preprocessing: Clean and preprocess the data to ensure it’s suitable for clustering. This may involve handling missing values, scaling features, or encoding categorical variables.\nFeature Selection: Choose relevant features that contribute to the clustering process. Dimensionality reduction techniques like PCA (Principal Component Analysis) can be employed to simplify complex datasets.\nHyperparameter Tuning: Tune the hyperparameters of the chosen clustering algorithm to achieve optimal results. This might involve experimenting with the number of clusters, distance metrics, or other algorithm-specific parameters.\nEvaluation Metrics: Define metrics to evaluate the performance of the clustering algorithm. Common metrics include silhouette score, Davies-Bouldin index, or visual inspection of cluster separation.\nData Visualization: Now, let’s bring our clustering results to life through a compelling visualization.\nThis blog demonstrates the clusters formed by the K-Means algorithm on a two-dimensional dataset. The colored points represent different clusters, and the red ‘X’ marks indicate the cluster centroids.\n\n\nData preprocessing\nThe popular Wine dataset is being used, which is included in scikit-learn. The dataset contains thirteen features, but for simplicity, we are using only the first two features in the visualization.\n\n\nK-means clustering:\nK-means clustering, a widely used partitioning method, seeks to organize a dataset into K clusters by assigning each data point to the cluster with the nearest mean. This iterative process refines clusters until convergence, providing valuable insights into inherent patterns. However, the effectiveness of K-means can be impeded by high-dimensional datasets, where numerous features contribute to increased computational complexity. In such cases, the application of Principal Component Analysis (PCA) as a preprocessing step becomes essential. PCA is a dimensionality reduction technique that transforms original features into a set of linearly uncorrelated variables, known as principal components. By focusing on the most significant dimensions, PCA retains crucial information while reducing the overall dimensionality, facilitating a more efficient clustering process.\nIn our specific use case, PCA is applied to the Wine dataset, reducing it to two principal components (n_components=2). This reduction simplifies the dataset while preserving a substantial portion of its variability. The reduced data is then fed into the K-means clustering algorithm. This two-step process of PCA followed by K-means offers several advantages, including overcoming the curse of dimensionality, providing a more interpretable representation, and enhancing the quality of resulting clusters. By concentrating on the directions of maximum variance, PCA helps to reveal and emphasize the most relevant information for clustering, ultimately contributing to more cohesive and well-defined clusters in a lower-dimensional space.\n\n\nVisualization:\nVisualizing the clusters helps us understand how well the algorithm has grouped the data. Let’s use a scatter plot to visualize the clusters and their centroids.\n\n# Import necessary libraries\nfrom sklearn.decomposition import PCA\nimport  numpy as np\nfrom  sklearn.cluster import KMeans\nfrom  sklearn import datasets\nfrom  sklearn.metrics import silhouette_score\nimport  matplotlib.pyplot as plt\n\n# Load the Wine dataset\nwine = datasets.load_wine()\ndata = wine.data\n\n# Apply PCA to reduce dimensionality\npca = PCA(n_components=2) \n # Choose the number of components (2 for visualization)\nreduced_data = pca.fit_transform(data)\n\n# Perform K-means clustering on the reduced data\nkmeans_pca = KMeans(n_clusters=3, random_state=42)\nkmeans_pca.fit(reduced_data)\nlabels_pca = kmeans_pca.labels_\ncenters_pca = kmeans_pca.cluster_centers_\n\n# Visualize the clusters in the reduced-dimensional space\nplt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels_pca, cmap='viridis', s=50, alpha=0.8, edgecolors='w')\nplt.scatter(centers_pca[:, 0], centers_pca[:, 1], c='red', marker='X', s=200, label='Cluster Centers')\nplt.title('K-means Clustering on Wine Dataset (PCA)')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nSilhouette Score:\nEvaluating Clustering Performance with Silhouette Score:\nThe silhouette score is a metric that quantifies how well-defined and separated the clusters are. It ranges from -1 to 1, where a high value indicates well-separated clusters. Let’s calculate the silhouette score for our K-means clustering.\n\n# Import silhouette_score\nfrom sklearn.metrics import silhouette_score\n\n# Calculate silhouette score for PCA-based clustering\nsilhouette_avg_pca = silhouette_score(reduced_data, labels_pca)\nprint(f\"Silhouette Score (PCA): {silhouette_avg_pca}\")\n\nSilhouette Score (PCA): 0.5722554756855064\n\n\n\n\nConclusion:\nClustering is a fascinating aspect of machine learning, enabling us to unveil patterns and structures within datasets. By understanding the requirements for implementing clustering algorithms and visualizing the results, we gain valuable insights into the underlying relationships within our data.\nReferences have been taken from various sources on the Internet."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blogs",
    "section": "",
    "text": "Classification Blog\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nTrisha Bora\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection Blog\n\n\n\n\n\n\n\nAnomaly Detection\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nTrisha Bora\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables Blog\n\n\n\n\n\n\n\nProbability_Random Variables\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nTrisha Bora\n\n\n\n\n\n\n  \n\n\n\n\nClustering Blog\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nTrisha Bora\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Non Linear Regression Blog\n\n\n\n\n\n\n\nRegression\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nTrisha Bora\n\n\n\n\n\n\nNo matching items"
  }
]